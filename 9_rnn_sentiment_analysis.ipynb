{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_rnn_sentiment_analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamdsc/deep_learning/blob/master/9_rnn_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5IOW7ETnotmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis on IMDb movie reviews using Multilayer RNN"
      ]
    },
    {
      "metadata": {
        "id": "1lJDvScjooFP",
        "colab_type": "code",
        "outputId": "056c38c3-fb04-4d5d-91d1-0e78605f5585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading the data from github\n",
        "!wget https://github.com/iamdsc/sentiment-analysis/raw/master/movie_data.csv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-27 01:10:41--  https://github.com/iamdsc/sentiment-analysis/raw/master/movie_data.csv\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/iamdsc/sentiment-analysis/master/movie_data.csv [following]\n",
            "--2019-02-27 01:10:41--  https://raw.githubusercontent.com/iamdsc/sentiment-analysis/master/movie_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65862309 (63M) [text/plain]\n",
            "Saving to: ‘movie_data.csv.3’\n",
            "\n",
            "movie_data.csv.3    100%[===================>]  62.81M   192MB/s    in 0.3s    \n",
            "\n",
            "2019-02-27 01:10:42 (192 MB/s) - ‘movie_data.csv.3’ saved [65862309/65862309]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7EZjzGaTpPph",
        "colab_type": "code",
        "outputId": "49137774-16f6-42d2-d4d8-219ac35c0563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "cell_type": "code",
      "source": [
        "# Preparing the data\n",
        "!pip install pyprind\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('movie_data.csv',encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyprind\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/30/e76fb0c45da8aef49ea8d2a90d4e7a6877b45894c25f12fb961f009a891e/PyPrind-2.11.2-py3-none-any.whl\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "07CiiJnrp6We",
        "colab_type": "code",
        "outputId": "6ee8d691-5b92-4867-90ba-45091396dd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing the data:\n",
        "# Seperate words and count each word's occurence\n",
        "from collections import Counter\n",
        "\n",
        "counts=Counter()\n",
        "pbar=pyprind.ProgBar(len(df['review']),title='Counting word occurences')\n",
        "for i, review in enumerate(df['review']):\n",
        "  text = ''.join([c if c not in punctuation else ' '+c+' ' for c in review]).lower()\n",
        "  df.loc[i,'review']=text\n",
        "  pbar.update()\n",
        "  counts.update(text.split())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting word occurences\n",
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:22\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTG88N8k9i84",
        "colab_type": "code",
        "outputId": "2c401aeb-fa8c-43c2-a114-d1db6d8e1160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "## Create a mapping\n",
        "## Map each unique word to an integer\n",
        "word_counts=sorted(counts,key=counts.get,reverse=True)\n",
        "print(word_counts[:5])\n",
        "word_to_int={word:ii for ii,word in enumerate(word_counts,1)}\n",
        "\n",
        "mapped_reviews=[]\n",
        "pbar=pyprind.ProgBar(len(df['review']),title='Map reviews to ints')\n",
        "for review in df['review']:\n",
        "  mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
        "  pbar.update()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Map reviews to ints\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['the', '.', ',', 'and', 'a']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:04\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JHP6t9cyAz9-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define same length sequences\n",
        "## if sequence length < 200: left-pad with zeros\n",
        "## if sequence length > 200: use last 200 elements\n",
        "\n",
        "sequence_length = 200 # known as T in our RNN formulas\n",
        "sequences = np.zeros((len(mapped_reviews),sequence_length),dtype=int)\n",
        "\n",
        "for i, row in enumerate(mapped_reviews):\n",
        "  review_arr=np.array(row)\n",
        "  sequences[i,-len(row):] = review_arr[-sequence_length:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uH6w31I5Ek43",
        "colab_type": "code",
        "outputId": "ad63ec00-0c88-4f68-b8ed-0b25c894e5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "sequences[:15,:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   37,  1956,  1801, ...,    85,    34,  1309],\n",
              "       [    4,  7431,   256, ...,     1,  1980,    41],\n",
              "       [  113,   587,     7, ...,   175,    29,     2],\n",
              "       ...,\n",
              "       [    3,  3353,     4, ...,  4019,    29,     2],\n",
              "       [ 1636,     7,    92, ...,     5, 14785,     2],\n",
              "       [    0,     0,     0, ...,   473,    29,     2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "uD0F9_OaMnZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# perform train-test split\n",
        "X_train = sequences[:25000,:]\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = sequences[25000:,:]\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O81zCWMDFZ3h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "# Define a function to generate mini batches:\n",
        "def create_batch_generator(x,y=None,batch_size=64):\n",
        "  n_batches=len(x)//batch_size\n",
        "  x=x[:n_batches*batch_size]\n",
        "  if y is not None:\n",
        "    y=y[:n_batches*batch_size]\n",
        "  for ii in range(0,len(x),batch_size):\n",
        "    if y is not None:\n",
        "      yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "    else:\n",
        "      yield x[ii:ii+batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdiQd3T9YnEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building an RNN Model"
      ]
    },
    {
      "metadata": {
        "id": "aebLylRLLyFD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class SentimentRNN(object):\n",
        "  def __init__(self,n_words,seq_len=200,lstm_size=256,num_layers=1,batch_size=64,learning_rate=0.0001,embed_size=200):\n",
        "    self.n_words=n_words\n",
        "    self.seq_len=seq_len\n",
        "    self.lstm_size=lstm_size  #number of hidden units\n",
        "    self.num_layers=num_layers\n",
        "    self.batch_size=batch_size\n",
        "    self.learning_rate=learning_rate\n",
        "    self.embed_size=embed_size\n",
        "    \n",
        "    self.g=tf.Graph()\n",
        "    with self.g.as_default():\n",
        "      tf.set_random_seed(123)\n",
        "      self.build()\n",
        "      self.saver=tf.train.Saver()\n",
        "      self.init_op=tf.global_variables_initializer()\n",
        "      \n",
        "  # the build method\n",
        "  def build(self):\n",
        "    # Define the placeholders\n",
        "    tf_x=tf.placeholder(tf.int32,shape=(self.batch_size,self.seq_len),name='tf_x')\n",
        "    tf_y=tf.placeholder(tf.float32,shape=(self.batch_size),name='tf_y')\n",
        "    tf_keepprob=tf.placeholder(tf.float32,name='tf_keepprob')\n",
        "    \n",
        "    # Create the embedding layer\n",
        "    embedding=tf.Variable(tf.random_uniform((self.n_words,self.embed_size),minval=-1,maxval=1),name='embedding')\n",
        "    embed_x=tf.nn.embedding_lookup(embedding,tf_x,name='embeded_x')\n",
        "    \n",
        "    # Define LSTM cell and stack them together\n",
        "    cells=tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(self.lstm_size),output_keep_prob=tf_keepprob) for i in range(self.num_layers)])\n",
        "    \n",
        "    # Define the initial state\n",
        "    self.initial_state=cells.zero_state(self.batch_size,tf.float32)\n",
        "    print(' << initial state >> ',self.initial_state)\n",
        "    \n",
        "    lstm_outputs, self.final_state=tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
        "    \n",
        "    # lstm_outputs shape: [batch_size,max_time,cells.output_size]\n",
        "    print('\\n <<lstm_output >> ',lstm_outputs)\n",
        "    print('\\n <<final_state >> ',self.final_state)\n",
        "    \n",
        "    logits=tf.layers.dense(inputs=lstm_outputs[:,-1],units=1,activation=None,name='logits')\n",
        "    logits=tf.squeeze(logits, name='logits_squeezed')\n",
        "    print('\\n << logits >> ',logits)\n",
        "    \n",
        "    y_proba=tf.nn.sigmoid(logits,name='probabilities')\n",
        "    predictions={'probabilties':y_proba,'labels':tf.cast(tf.round(y_proba),tf.int32,name='labels')}\n",
        "    print('\\n << predictions >> ',predictions)\n",
        "    \n",
        "    # Define the cost function\n",
        "    cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y,logits=logits),name='cost')\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer=tf.train.AdamOptimizer(self.learning_rate)\n",
        "    train_op=optimizer.minimize(cost,name='train_op')\n",
        "    \n",
        "  def train(self, X_train, y_train, num_epochs):\n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      sess.run(self.init_op)\n",
        "      iteration=1\n",
        "      for epoch in range(num_epochs):\n",
        "        state=sess.run(self.initial_state)\n",
        "        for batch_x,batch_y in create_batch_generator(X_train,y_train,self.batch_size):\n",
        "          feed={'tf_x:0':batch_x,'tf_y:0':batch_y,'tf_keepprob:0':0.5,self.initial_state:state}\n",
        "          loss,_,state=sess.run(['cost:0','train_op',self.final_state],feed_dict=feed)\n",
        "          if iteration % 20 == 0:\n",
        "            print('Epoch: %d/%d Iteration: %d | Train loss: %.5f' % (epoch+1,num_epochs,iteration,loss))\n",
        "          iteration+=1\n",
        "          if(epoch+1)%10 == 0:\n",
        "            self.saver.save(sess,'model/sentiment-%d.ckpt'%epoch)\n",
        "  \n",
        "  def predict(self,X_data,return_proba=False):\n",
        "    preds=[]\n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      self.saver.restore(sess,tf.train.latest_checkpoint('./model/'))\n",
        "      test_state=sess.run(self.initial_state)\n",
        "      for ii, batch_x in enumerate(create_batch_generator(X_data, None, batch_size=self.batch_size),1):\n",
        "        feed = {'tf_x:0' : batch_x,'tf_keepprob:0' : 1.0,self.initial_state : test_state}\n",
        "        if return_proba:\n",
        "          pred,test_state=sess.run(['probabilities:0',self.final_state],feed_dict=feed)\n",
        "        else:\n",
        "          pred,test_state=sess.run(['labels:0',self.final_state],feed_dict=feed)\n",
        "          preds.append(pred)\n",
        "        \n",
        "    return np.concatenate(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pz1Ip0GQcg2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9900
        },
        "outputId": "9a179c9c-24f9-4b66-ff96-28ae4ddf0ca3"
      },
      "cell_type": "code",
      "source": [
        "# Instantiating the SentimentRNN Class\n",
        "n_words=max(list(word_to_int.values()))+1\n",
        "\n",
        "rnn=SentimentRNN(n_words=n_words,seq_len=sequence_length,embed_size=256,lstm_size=128,num_layers=1,batch_size=100,learning_rate=0.001)\n",
        "\n",
        "# Train the model\n",
        "rnn.train(X_train, y_train, num_epochs=40)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-14-03c896de10f2>:32: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-14-03c896de10f2>:32: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
            "WARNING:tensorflow:From <ipython-input-14-03c896de10f2>:38: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "\n",
            " <<lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
            "\n",
            " <<final_state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
            "WARNING:tensorflow:From <ipython-input-14-03c896de10f2>:44: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "\n",
            " << logits >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
            "\n",
            " << predictions >>  {'probabilties': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch: 1/40 Iteration: 20 | Train loss: 0.68492\n",
            "Epoch: 1/40 Iteration: 40 | Train loss: 0.56067\n",
            "Epoch: 1/40 Iteration: 60 | Train loss: 0.66468\n",
            "Epoch: 1/40 Iteration: 80 | Train loss: 0.54809\n",
            "Epoch: 1/40 Iteration: 100 | Train loss: 0.55514\n",
            "Epoch: 1/40 Iteration: 120 | Train loss: 0.46886\n",
            "Epoch: 1/40 Iteration: 140 | Train loss: 0.49383\n",
            "Epoch: 1/40 Iteration: 160 | Train loss: 0.42235\n",
            "Epoch: 1/40 Iteration: 180 | Train loss: 0.46936\n",
            "Epoch: 1/40 Iteration: 200 | Train loss: 0.49222\n",
            "Epoch: 1/40 Iteration: 220 | Train loss: 0.46515\n",
            "Epoch: 1/40 Iteration: 240 | Train loss: 0.40949\n",
            "Epoch: 2/40 Iteration: 260 | Train loss: 0.48731\n",
            "Epoch: 2/40 Iteration: 280 | Train loss: 0.29965\n",
            "Epoch: 2/40 Iteration: 300 | Train loss: 0.35869\n",
            "Epoch: 2/40 Iteration: 320 | Train loss: 0.39321\n",
            "Epoch: 2/40 Iteration: 340 | Train loss: 0.33521\n",
            "Epoch: 2/40 Iteration: 360 | Train loss: 0.21717\n",
            "Epoch: 2/40 Iteration: 380 | Train loss: 0.35689\n",
            "Epoch: 2/40 Iteration: 400 | Train loss: 0.38720\n",
            "Epoch: 2/40 Iteration: 420 | Train loss: 0.34945\n",
            "Epoch: 2/40 Iteration: 440 | Train loss: 0.34494\n",
            "Epoch: 2/40 Iteration: 460 | Train loss: 0.46413\n",
            "Epoch: 2/40 Iteration: 480 | Train loss: 0.24813\n",
            "Epoch: 2/40 Iteration: 500 | Train loss: 0.27754\n",
            "Epoch: 3/40 Iteration: 520 | Train loss: 0.25001\n",
            "Epoch: 3/40 Iteration: 540 | Train loss: 0.25412\n",
            "Epoch: 3/40 Iteration: 560 | Train loss: 0.32468\n",
            "Epoch: 3/40 Iteration: 580 | Train loss: 0.24004\n",
            "Epoch: 3/40 Iteration: 600 | Train loss: 0.23203\n",
            "Epoch: 3/40 Iteration: 620 | Train loss: 0.21599\n",
            "Epoch: 3/40 Iteration: 640 | Train loss: 0.25925\n",
            "Epoch: 3/40 Iteration: 660 | Train loss: 0.14357\n",
            "Epoch: 3/40 Iteration: 680 | Train loss: 0.32014\n",
            "Epoch: 3/40 Iteration: 700 | Train loss: 0.26900\n",
            "Epoch: 3/40 Iteration: 720 | Train loss: 0.20872\n",
            "Epoch: 3/40 Iteration: 740 | Train loss: 0.22349\n",
            "Epoch: 4/40 Iteration: 760 | Train loss: 0.24192\n",
            "Epoch: 4/40 Iteration: 780 | Train loss: 0.17184\n",
            "Epoch: 4/40 Iteration: 800 | Train loss: 0.15100\n",
            "Epoch: 4/40 Iteration: 820 | Train loss: 0.25313\n",
            "Epoch: 4/40 Iteration: 840 | Train loss: 0.13130\n",
            "Epoch: 4/40 Iteration: 860 | Train loss: 0.10332\n",
            "Epoch: 4/40 Iteration: 880 | Train loss: 0.32458\n",
            "Epoch: 4/40 Iteration: 900 | Train loss: 0.20357\n",
            "Epoch: 4/40 Iteration: 920 | Train loss: 0.16322\n",
            "Epoch: 4/40 Iteration: 940 | Train loss: 0.24588\n",
            "Epoch: 4/40 Iteration: 960 | Train loss: 0.26465\n",
            "Epoch: 4/40 Iteration: 980 | Train loss: 0.15758\n",
            "Epoch: 4/40 Iteration: 1000 | Train loss: 0.14207\n",
            "Epoch: 5/40 Iteration: 1020 | Train loss: 0.13374\n",
            "Epoch: 5/40 Iteration: 1040 | Train loss: 0.15293\n",
            "Epoch: 5/40 Iteration: 1060 | Train loss: 0.19116\n",
            "Epoch: 5/40 Iteration: 1080 | Train loss: 0.09381\n",
            "Epoch: 5/40 Iteration: 1100 | Train loss: 0.10657\n",
            "Epoch: 5/40 Iteration: 1120 | Train loss: 0.20204\n",
            "Epoch: 5/40 Iteration: 1140 | Train loss: 0.12148\n",
            "Epoch: 5/40 Iteration: 1160 | Train loss: 0.10953\n",
            "Epoch: 5/40 Iteration: 1180 | Train loss: 0.19680\n",
            "Epoch: 5/40 Iteration: 1200 | Train loss: 0.05755\n",
            "Epoch: 5/40 Iteration: 1220 | Train loss: 0.09781\n",
            "Epoch: 5/40 Iteration: 1240 | Train loss: 0.22140\n",
            "Epoch: 6/40 Iteration: 1260 | Train loss: 0.29071\n",
            "Epoch: 6/40 Iteration: 1280 | Train loss: 0.05017\n",
            "Epoch: 6/40 Iteration: 1300 | Train loss: 0.07607\n",
            "Epoch: 6/40 Iteration: 1320 | Train loss: 0.10572\n",
            "Epoch: 6/40 Iteration: 1340 | Train loss: 0.04010\n",
            "Epoch: 6/40 Iteration: 1360 | Train loss: 0.09382\n",
            "Epoch: 6/40 Iteration: 1380 | Train loss: 0.28493\n",
            "Epoch: 6/40 Iteration: 1400 | Train loss: 0.15528\n",
            "Epoch: 6/40 Iteration: 1420 | Train loss: 0.10909\n",
            "Epoch: 6/40 Iteration: 1440 | Train loss: 0.09967\n",
            "Epoch: 6/40 Iteration: 1460 | Train loss: 0.20125\n",
            "Epoch: 6/40 Iteration: 1480 | Train loss: 0.19871\n",
            "Epoch: 6/40 Iteration: 1500 | Train loss: 0.07327\n",
            "Epoch: 7/40 Iteration: 1520 | Train loss: 0.13565\n",
            "Epoch: 7/40 Iteration: 1540 | Train loss: 0.03602\n",
            "Epoch: 7/40 Iteration: 1560 | Train loss: 0.08471\n",
            "Epoch: 7/40 Iteration: 1580 | Train loss: 0.09228\n",
            "Epoch: 7/40 Iteration: 1600 | Train loss: 0.16496\n",
            "Epoch: 7/40 Iteration: 1620 | Train loss: 0.07827\n",
            "Epoch: 7/40 Iteration: 1640 | Train loss: 0.04641\n",
            "Epoch: 7/40 Iteration: 1660 | Train loss: 0.04423\n",
            "Epoch: 7/40 Iteration: 1680 | Train loss: 0.08126\n",
            "Epoch: 7/40 Iteration: 1700 | Train loss: 0.08153\n",
            "Epoch: 7/40 Iteration: 1720 | Train loss: 0.04629\n",
            "Epoch: 7/40 Iteration: 1740 | Train loss: 0.09436\n",
            "Epoch: 8/40 Iteration: 1760 | Train loss: 0.10454\n",
            "Epoch: 8/40 Iteration: 1780 | Train loss: 0.02301\n",
            "Epoch: 8/40 Iteration: 1800 | Train loss: 0.03041\n",
            "Epoch: 8/40 Iteration: 1820 | Train loss: 0.04460\n",
            "Epoch: 8/40 Iteration: 1840 | Train loss: 0.01760\n",
            "Epoch: 8/40 Iteration: 1860 | Train loss: 0.02197\n",
            "Epoch: 8/40 Iteration: 1880 | Train loss: 0.07471\n",
            "Epoch: 8/40 Iteration: 1900 | Train loss: 0.06322\n",
            "Epoch: 8/40 Iteration: 1920 | Train loss: 0.06335\n",
            "Epoch: 8/40 Iteration: 1940 | Train loss: 0.01364\n",
            "Epoch: 8/40 Iteration: 1960 | Train loss: 0.11949\n",
            "Epoch: 8/40 Iteration: 1980 | Train loss: 0.11530\n",
            "Epoch: 8/40 Iteration: 2000 | Train loss: 0.04584\n",
            "Epoch: 9/40 Iteration: 2020 | Train loss: 0.06410\n",
            "Epoch: 9/40 Iteration: 2040 | Train loss: 0.01161\n",
            "Epoch: 9/40 Iteration: 2060 | Train loss: 0.02266\n",
            "Epoch: 9/40 Iteration: 2080 | Train loss: 0.05509\n",
            "Epoch: 9/40 Iteration: 2100 | Train loss: 0.06217\n",
            "Epoch: 9/40 Iteration: 2120 | Train loss: 0.08673\n",
            "Epoch: 9/40 Iteration: 2140 | Train loss: 0.01404\n",
            "Epoch: 9/40 Iteration: 2160 | Train loss: 0.01030\n",
            "Epoch: 9/40 Iteration: 2180 | Train loss: 0.01582\n",
            "Epoch: 9/40 Iteration: 2200 | Train loss: 0.08579\n",
            "Epoch: 9/40 Iteration: 2220 | Train loss: 0.05980\n",
            "Epoch: 9/40 Iteration: 2240 | Train loss: 0.10144\n",
            "Epoch: 10/40 Iteration: 2260 | Train loss: 0.04496\n",
            "Epoch: 10/40 Iteration: 2280 | Train loss: 0.01377\n",
            "Epoch: 10/40 Iteration: 2300 | Train loss: 0.00710\n",
            "Epoch: 10/40 Iteration: 2320 | Train loss: 0.00858\n",
            "Epoch: 10/40 Iteration: 2340 | Train loss: 0.02302\n",
            "Epoch: 10/40 Iteration: 2360 | Train loss: 0.00922\n",
            "Epoch: 10/40 Iteration: 2380 | Train loss: 0.06174\n",
            "Epoch: 10/40 Iteration: 2400 | Train loss: 0.03950\n",
            "Epoch: 10/40 Iteration: 2420 | Train loss: 0.04371\n",
            "Epoch: 10/40 Iteration: 2440 | Train loss: 0.02132\n",
            "Epoch: 10/40 Iteration: 2460 | Train loss: 0.00973\n",
            "Epoch: 10/40 Iteration: 2480 | Train loss: 0.02909\n",
            "Epoch: 10/40 Iteration: 2500 | Train loss: 0.05019\n",
            "Epoch: 11/40 Iteration: 2520 | Train loss: 0.01724\n",
            "Epoch: 11/40 Iteration: 2540 | Train loss: 0.00511\n",
            "Epoch: 11/40 Iteration: 2560 | Train loss: 0.00730\n",
            "Epoch: 11/40 Iteration: 2580 | Train loss: 0.03315\n",
            "Epoch: 11/40 Iteration: 2600 | Train loss: 0.00165\n",
            "Epoch: 11/40 Iteration: 2620 | Train loss: 0.07371\n",
            "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00386\n",
            "Epoch: 11/40 Iteration: 2660 | Train loss: 0.00844\n",
            "Epoch: 11/40 Iteration: 2680 | Train loss: 0.00370\n",
            "Epoch: 11/40 Iteration: 2700 | Train loss: 0.01123\n",
            "Epoch: 11/40 Iteration: 2720 | Train loss: 0.00231\n",
            "Epoch: 11/40 Iteration: 2740 | Train loss: 0.02602\n",
            "Epoch: 12/40 Iteration: 2760 | Train loss: 0.00234\n",
            "Epoch: 12/40 Iteration: 2780 | Train loss: 0.00543\n",
            "Epoch: 12/40 Iteration: 2800 | Train loss: 0.00611\n",
            "Epoch: 12/40 Iteration: 2820 | Train loss: 0.00225\n",
            "Epoch: 12/40 Iteration: 2840 | Train loss: 0.01038\n",
            "Epoch: 12/40 Iteration: 2860 | Train loss: 0.01050\n",
            "Epoch: 12/40 Iteration: 2880 | Train loss: 0.01503\n",
            "Epoch: 12/40 Iteration: 2900 | Train loss: 0.04049\n",
            "Epoch: 12/40 Iteration: 2920 | Train loss: 0.05500\n",
            "Epoch: 12/40 Iteration: 2940 | Train loss: 0.01173\n",
            "Epoch: 12/40 Iteration: 2960 | Train loss: 0.00085\n",
            "Epoch: 12/40 Iteration: 2980 | Train loss: 0.07079\n",
            "Epoch: 12/40 Iteration: 3000 | Train loss: 0.03265\n",
            "Epoch: 13/40 Iteration: 3020 | Train loss: 0.02240\n",
            "Epoch: 13/40 Iteration: 3040 | Train loss: 0.00507\n",
            "Epoch: 13/40 Iteration: 3060 | Train loss: 0.01373\n",
            "Epoch: 13/40 Iteration: 3080 | Train loss: 0.00195\n",
            "Epoch: 13/40 Iteration: 3100 | Train loss: 0.00742\n",
            "Epoch: 13/40 Iteration: 3120 | Train loss: 0.09030\n",
            "Epoch: 13/40 Iteration: 3140 | Train loss: 0.02321\n",
            "Epoch: 13/40 Iteration: 3160 | Train loss: 0.00208\n",
            "Epoch: 13/40 Iteration: 3180 | Train loss: 0.00284\n",
            "Epoch: 13/40 Iteration: 3200 | Train loss: 0.00373\n",
            "Epoch: 13/40 Iteration: 3220 | Train loss: 0.01105\n",
            "Epoch: 13/40 Iteration: 3240 | Train loss: 0.02334\n",
            "Epoch: 14/40 Iteration: 3260 | Train loss: 0.00638\n",
            "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00326\n",
            "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00702\n",
            "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00246\n",
            "Epoch: 14/40 Iteration: 3340 | Train loss: 0.00388\n",
            "Epoch: 14/40 Iteration: 3360 | Train loss: 0.00142\n",
            "Epoch: 14/40 Iteration: 3380 | Train loss: 0.00346\n",
            "Epoch: 14/40 Iteration: 3400 | Train loss: 0.04495\n",
            "Epoch: 14/40 Iteration: 3420 | Train loss: 0.02394\n",
            "Epoch: 14/40 Iteration: 3440 | Train loss: 0.00449\n",
            "Epoch: 14/40 Iteration: 3460 | Train loss: 0.02901\n",
            "Epoch: 14/40 Iteration: 3480 | Train loss: 0.00206\n",
            "Epoch: 14/40 Iteration: 3500 | Train loss: 0.00059\n",
            "Epoch: 15/40 Iteration: 3520 | Train loss: 0.00212\n",
            "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00086\n",
            "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00200\n",
            "Epoch: 15/40 Iteration: 3580 | Train loss: 0.00292\n",
            "Epoch: 15/40 Iteration: 3600 | Train loss: 0.00354\n",
            "Epoch: 15/40 Iteration: 3620 | Train loss: 0.06553\n",
            "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00111\n",
            "Epoch: 15/40 Iteration: 3660 | Train loss: 0.00153\n",
            "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00831\n",
            "Epoch: 15/40 Iteration: 3700 | Train loss: 0.03051\n",
            "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00680\n",
            "Epoch: 15/40 Iteration: 3740 | Train loss: 0.02438\n",
            "Epoch: 16/40 Iteration: 3760 | Train loss: 0.05567\n",
            "Epoch: 16/40 Iteration: 3780 | Train loss: 0.01140\n",
            "Epoch: 16/40 Iteration: 3800 | Train loss: 0.00490\n",
            "Epoch: 16/40 Iteration: 3820 | Train loss: 0.09635\n",
            "Epoch: 16/40 Iteration: 3840 | Train loss: 0.00335\n",
            "Epoch: 16/40 Iteration: 3860 | Train loss: 0.01165\n",
            "Epoch: 16/40 Iteration: 3880 | Train loss: 0.02000\n",
            "Epoch: 16/40 Iteration: 3900 | Train loss: 0.01608\n",
            "Epoch: 16/40 Iteration: 3920 | Train loss: 0.01446\n",
            "Epoch: 16/40 Iteration: 3940 | Train loss: 0.00929\n",
            "Epoch: 16/40 Iteration: 3960 | Train loss: 0.00579\n",
            "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00128\n",
            "Epoch: 16/40 Iteration: 4000 | Train loss: 0.00261\n",
            "Epoch: 17/40 Iteration: 4020 | Train loss: 0.01301\n",
            "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00336\n",
            "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00644\n",
            "Epoch: 17/40 Iteration: 4080 | Train loss: 0.04801\n",
            "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00594\n",
            "Epoch: 17/40 Iteration: 4120 | Train loss: 0.10762\n",
            "Epoch: 17/40 Iteration: 4140 | Train loss: 0.00362\n",
            "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00645\n",
            "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00124\n",
            "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00033\n",
            "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00101\n",
            "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00993\n",
            "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00159\n",
            "Epoch: 18/40 Iteration: 4280 | Train loss: 0.00095\n",
            "Epoch: 18/40 Iteration: 4300 | Train loss: 0.01365\n",
            "Epoch: 18/40 Iteration: 4320 | Train loss: 0.05358\n",
            "Epoch: 18/40 Iteration: 4340 | Train loss: 0.04516\n",
            "Epoch: 18/40 Iteration: 4360 | Train loss: 0.04159\n",
            "Epoch: 18/40 Iteration: 4380 | Train loss: 0.00557\n",
            "Epoch: 18/40 Iteration: 4400 | Train loss: 0.01107\n",
            "Epoch: 18/40 Iteration: 4420 | Train loss: 0.07514\n",
            "Epoch: 18/40 Iteration: 4440 | Train loss: 0.03591\n",
            "Epoch: 18/40 Iteration: 4460 | Train loss: 0.00179\n",
            "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00152\n",
            "Epoch: 18/40 Iteration: 4500 | Train loss: 0.01102\n",
            "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00873\n",
            "Epoch: 19/40 Iteration: 4540 | Train loss: 0.01705\n",
            "Epoch: 19/40 Iteration: 4560 | Train loss: 0.07001\n",
            "Epoch: 19/40 Iteration: 4580 | Train loss: 0.01090\n",
            "Epoch: 19/40 Iteration: 4600 | Train loss: 0.00138\n",
            "Epoch: 19/40 Iteration: 4620 | Train loss: 0.03292\n",
            "Epoch: 19/40 Iteration: 4640 | Train loss: 0.00061\n",
            "Epoch: 19/40 Iteration: 4660 | Train loss: 0.00062\n",
            "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00474\n",
            "Epoch: 19/40 Iteration: 4700 | Train loss: 0.00152\n",
            "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00151\n",
            "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00066\n",
            "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00111\n",
            "Epoch: 20/40 Iteration: 4780 | Train loss: 0.00091\n",
            "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00132\n",
            "Epoch: 20/40 Iteration: 4820 | Train loss: 0.00140\n",
            "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00097\n",
            "Epoch: 20/40 Iteration: 4860 | Train loss: 0.00080\n",
            "Epoch: 20/40 Iteration: 4880 | Train loss: 0.00167\n",
            "Epoch: 20/40 Iteration: 4900 | Train loss: 0.00084\n",
            "Epoch: 20/40 Iteration: 4920 | Train loss: 0.00157\n",
            "Epoch: 20/40 Iteration: 4940 | Train loss: 0.00171\n",
            "Epoch: 20/40 Iteration: 4960 | Train loss: 0.00089\n",
            "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00426\n",
            "Epoch: 20/40 Iteration: 5000 | Train loss: 0.00513\n",
            "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00024\n",
            "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00077\n",
            "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00219\n",
            "Epoch: 21/40 Iteration: 5080 | Train loss: 0.00243\n",
            "Epoch: 21/40 Iteration: 5100 | Train loss: 0.00040\n",
            "Epoch: 21/40 Iteration: 5120 | Train loss: 0.01820\n",
            "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00108\n",
            "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00140\n",
            "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00037\n",
            "Epoch: 21/40 Iteration: 5200 | Train loss: 0.00083\n",
            "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00032\n",
            "Epoch: 21/40 Iteration: 5240 | Train loss: 0.00226\n",
            "Epoch: 22/40 Iteration: 5260 | Train loss: 0.00034\n",
            "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00017\n",
            "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00029\n",
            "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00257\n",
            "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00473\n",
            "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00078\n",
            "Epoch: 22/40 Iteration: 5380 | Train loss: 0.00230\n",
            "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00055\n",
            "Epoch: 22/40 Iteration: 5420 | Train loss: 0.04015\n",
            "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00510\n",
            "Epoch: 22/40 Iteration: 5460 | Train loss: 0.01114\n",
            "Epoch: 22/40 Iteration: 5480 | Train loss: 0.01408\n",
            "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00132\n",
            "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00833\n",
            "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00043\n",
            "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00789\n",
            "Epoch: 23/40 Iteration: 5580 | Train loss: 0.01399\n",
            "Epoch: 23/40 Iteration: 5600 | Train loss: 0.03456\n",
            "Epoch: 23/40 Iteration: 5620 | Train loss: 0.01020\n",
            "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00243\n",
            "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00170\n",
            "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00218\n",
            "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00058\n",
            "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00043\n",
            "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00072\n",
            "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00022\n",
            "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00028\n",
            "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00322\n",
            "Epoch: 24/40 Iteration: 5820 | Train loss: 0.05264\n",
            "Epoch: 24/40 Iteration: 5840 | Train loss: 0.00072\n",
            "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00010\n",
            "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00210\n",
            "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00038\n",
            "Epoch: 24/40 Iteration: 5920 | Train loss: 0.02600\n",
            "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00028\n",
            "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00017\n",
            "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00031\n",
            "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00019\n",
            "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00471\n",
            "Epoch: 25/40 Iteration: 6040 | Train loss: 0.01142\n",
            "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00366\n",
            "Epoch: 25/40 Iteration: 6080 | Train loss: 0.03839\n",
            "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00068\n",
            "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00315\n",
            "Epoch: 25/40 Iteration: 6140 | Train loss: 0.01022\n",
            "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00086\n",
            "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00028\n",
            "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00075\n",
            "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00036\n",
            "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00246\n",
            "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00058\n",
            "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00207\n",
            "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00798\n",
            "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00391\n",
            "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00181\n",
            "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00026\n",
            "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00035\n",
            "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00157\n",
            "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00035\n",
            "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00080\n",
            "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00041\n",
            "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00016\n",
            "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00093\n",
            "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00013\n",
            "Epoch: 27/40 Iteration: 6540 | Train loss: 0.12020\n",
            "Epoch: 27/40 Iteration: 6560 | Train loss: 0.01160\n",
            "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00022\n",
            "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00159\n",
            "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00104\n",
            "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00017\n",
            "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00017\n",
            "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00883\n",
            "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00091\n",
            "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00094\n",
            "Epoch: 27/40 Iteration: 6740 | Train loss: 0.02218\n",
            "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00132\n",
            "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00106\n",
            "Epoch: 28/40 Iteration: 6800 | Train loss: 0.04131\n",
            "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00028\n",
            "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00034\n",
            "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00031\n",
            "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00065\n",
            "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00056\n",
            "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00088\n",
            "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00410\n",
            "Epoch: 28/40 Iteration: 6960 | Train loss: 0.09420\n",
            "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00114\n",
            "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00059\n",
            "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00027\n",
            "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00071\n",
            "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00127\n",
            "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00025\n",
            "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00053\n",
            "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00213\n",
            "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00016\n",
            "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00019\n",
            "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00037\n",
            "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00011\n",
            "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00022\n",
            "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00037\n",
            "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00030\n",
            "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00146\n",
            "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00048\n",
            "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00022\n",
            "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00008\n",
            "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00014\n",
            "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00008\n",
            "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00046\n",
            "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00007\n",
            "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00007\n",
            "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00040\n",
            "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00006\n",
            "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00010\n",
            "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00009\n",
            "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00021\n",
            "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00025\n",
            "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00010\n",
            "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00010\n",
            "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00008\n",
            "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00021\n",
            "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00006\n",
            "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00008\n",
            "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00010\n",
            "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00006\n",
            "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00018\n",
            "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00005\n",
            "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00003\n",
            "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00022\n",
            "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00003\n",
            "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00020\n",
            "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00161\n",
            "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00009\n",
            "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00007\n",
            "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00008\n",
            "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00006\n",
            "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00009\n",
            "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00318\n",
            "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00008\n",
            "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00003\n",
            "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00029\n",
            "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00005\n",
            "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00006\n",
            "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00016\n",
            "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00019\n",
            "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00006\n",
            "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00026\n",
            "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00002\n",
            "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00009\n",
            "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00004\n",
            "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00002\n",
            "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00006\n",
            "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00003\n",
            "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00098\n",
            "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00007\n",
            "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00003\n",
            "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00003\n",
            "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00006\n",
            "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00002\n",
            "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00003\n",
            "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00005\n",
            "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00006\n",
            "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00007\n",
            "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00179\n",
            "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00007\n",
            "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00002\n",
            "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00005\n",
            "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00031\n",
            "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00004\n",
            "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00005\n",
            "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00003\n",
            "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00008\n",
            "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00009\n",
            "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00003\n",
            "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9420 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9460 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00003\n",
            "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00004\n",
            "Epoch: 39/40 Iteration: 9580 | Train loss: 0.00003\n",
            "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00002\n",
            "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00005\n",
            "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00023\n",
            "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-5hNfpYxdk5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "adc05296-0693-4ab1-fd8a-63943840f47a"
      },
      "cell_type": "code",
      "source": [
        "# Performing prediction\n",
        "\n",
        "preds=rnn.predict(X_test)\n",
        "y_true=y_test[:len(preds)]\n",
        "print('Test Acc.: %.3f'%(100*np.sum(preds==y_true)/len(y_true)))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/sentiment-39.ckpt\n",
            "Test Acc.: 85.716\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}