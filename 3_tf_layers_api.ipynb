{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Neural Network using Tf's Layers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from path\"\"\"\n",
    "    labels_path=os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path=os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n=struct.unpack('>II', lbpath.read(8))\n",
    "        labels=np.fromfile(lbpath, dtype=np.uint8)\n",
    "    \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols=struct.unpack('>IIII', imgpath.read(16))\n",
    "        images=np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images=((images/255.0)-0.5)*2 #normalized to -1 to 1 range\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "X_train, y_train=load_mnist('./data/',kind='train')\n",
    "print('Rows: %d, Columns: %d'%(X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, Columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test=load_mnist('./data/',kind='t10k')\n",
    "print('Rows: %d, Columns: %d'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean centering and normalization\n",
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val=np.std(X_train)\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=X_train_centered.shape[1]\n",
    "n_classes=10\n",
    "random_seed=123\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.set_random_seed(random_seed)\n",
    "    tf_x=tf.placeholder(dtype=tf.float32, shape=(None, n_features), name='tf_x')\n",
    "    tf_y=tf.placeholder(dtype=tf.int32, shape=None, name='tf_y')\n",
    "    y_onehot=tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "    h1=tf.layers.dense(inputs=tf_x, units=50, activation=tf.tanh, name='layer1')\n",
    "    h2=tf.layers.dense(inputs=h1, units=50, activation=tf.tanh, name='layer2')\n",
    "    logits=tf.layers.dense(inputs=h2, units=10, activation=None, name='layer3')\n",
    "    predictions={'classes':tf.argmax(logits, axis=1, name='predicted_classes'), 'probabilities':tf.nn.softmax(logits, name='softmax_tensor')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining cost function and an optimizer\n",
    "with g.as_default():\n",
    "    cost=tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=logits)\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op=optimizer.minimize(loss=cost)\n",
    "    init_op=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating batches of data\n",
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy=np.array(X)\n",
    "    y_copy=np.array(y)\n",
    "    if shuffle:\n",
    "        data=np.column_stack((X_copy, y_copy))\n",
    "        np.shuffle(data)\n",
    "        X_copy=data[:,:-1]\n",
    "        y_copy=data[:,-1].astype(int)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield(X_copy[i:i+batch_size,:], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Epoch  1 Avg. Training Loss: 1.5573\n",
      "--Epoch  2 Avg. Training Loss: 0.9492\n",
      "--Epoch  3 Avg. Training Loss: 0.7499\n",
      "--Epoch  4 Avg. Training Loss: 0.6387\n",
      "--Epoch  5 Avg. Training Loss: 0.5668\n",
      "--Epoch  6 Avg. Training Loss: 0.5160\n",
      "--Epoch  7 Avg. Training Loss: 0.4781\n",
      "--Epoch  8 Avg. Training Loss: 0.4486\n",
      "--Epoch  9 Avg. Training Loss: 0.4247\n",
      "--Epoch 10 Avg. Training Loss: 0.4051\n",
      "--Epoch 11 Avg. Training Loss: 0.3884\n",
      "--Epoch 12 Avg. Training Loss: 0.3741\n",
      "--Epoch 13 Avg. Training Loss: 0.3617\n",
      "--Epoch 14 Avg. Training Loss: 0.3507\n",
      "--Epoch 15 Avg. Training Loss: 0.3408\n",
      "--Epoch 16 Avg. Training Loss: 0.3320\n",
      "--Epoch 17 Avg. Training Loss: 0.3239\n",
      "--Epoch 18 Avg. Training Loss: 0.3165\n",
      "--Epoch 19 Avg. Training Loss: 0.3097\n",
      "--Epoch 20 Avg. Training Loss: 0.3035\n",
      "--Epoch 21 Avg. Training Loss: 0.2976\n",
      "--Epoch 22 Avg. Training Loss: 0.2921\n",
      "--Epoch 23 Avg. Training Loss: 0.2870\n",
      "--Epoch 24 Avg. Training Loss: 0.2822\n",
      "--Epoch 25 Avg. Training Loss: 0.2776\n",
      "--Epoch 26 Avg. Training Loss: 0.2733\n",
      "--Epoch 27 Avg. Training Loss: 0.2693\n",
      "--Epoch 28 Avg. Training Loss: 0.2654\n",
      "--Epoch 29 Avg. Training Loss: 0.2617\n",
      "--Epoch 30 Avg. Training Loss: 0.2581\n",
      "--Epoch 31 Avg. Training Loss: 0.2547\n",
      "--Epoch 32 Avg. Training Loss: 0.2515\n",
      "--Epoch 33 Avg. Training Loss: 0.2483\n",
      "--Epoch 34 Avg. Training Loss: 0.2453\n",
      "--Epoch 35 Avg. Training Loss: 0.2425\n",
      "--Epoch 36 Avg. Training Loss: 0.2397\n",
      "--Epoch 37 Avg. Training Loss: 0.2370\n",
      "--Epoch 38 Avg. Training Loss: 0.2344\n",
      "--Epoch 39 Avg. Training Loss: 0.2319\n",
      "--Epoch 40 Avg. Training Loss: 0.2294\n",
      "--Epoch 41 Avg. Training Loss: 0.2271\n",
      "--Epoch 42 Avg. Training Loss: 0.2248\n",
      "--Epoch 43 Avg. Training Loss: 0.2226\n",
      "--Epoch 44 Avg. Training Loss: 0.2204\n",
      "--Epoch 45 Avg. Training Loss: 0.2183\n",
      "--Epoch 46 Avg. Training Loss: 0.2163\n",
      "--Epoch 47 Avg. Training Loss: 0.2143\n",
      "--Epoch 48 Avg. Training Loss: 0.2124\n",
      "--Epoch 49 Avg. Training Loss: 0.2105\n",
      "--Epoch 50 Avg. Training Loss: 0.2086\n"
     ]
    }
   ],
   "source": [
    "# Create a session to launch the graph\n",
    "sess=tf.Session(graph=g)\n",
    "# Run the variable initialization operator\n",
    "sess.run(init_op)\n",
    "\n",
    "# 50 epochs of training\n",
    "for epoch in range(50):\n",
    "    training_costs=[]\n",
    "    batch_generator=create_batch_generator(X_train_centered, y_train, batch_size=64)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        # Prepare dict to feed data to network\n",
    "        feed={tf_x:batch_X, tf_y:batch_y}\n",
    "        _, batch_cost=sess.run([train_op, cost], feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print('--Epoch %2d Avg. Training Loss: %.4f'%(epoch+1, np.mean(training_costs)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.89%\n"
     ]
    }
   ],
   "source": [
    "# Performing predictions on the test dataset\n",
    "feed={tf_x:X_test_centered}\n",
    "y_pred=sess.run(predictions['classes'], feed_dict=feed)\n",
    "print('Test Accuracy: %.2f%%'%(100*np.sum(y_pred==y_test)/y_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
